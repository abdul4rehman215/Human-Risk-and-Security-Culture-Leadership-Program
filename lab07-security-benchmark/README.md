# ðŸ§ª Lab 7: Benchmarking Your Security Program

## ðŸ“Œ Overview
This lab builds a **Security Program Maturity Benchmarking** toolkit using a weighted maturity model across key security domains (governance, risk, IR, awareness, and technical controls).  
Youâ€™ll create a benchmarking environment, define an assessment framework in YAML, run maturity scoring, and generate reports + charts.

---

## ðŸŽ¯ Objectives
By the end of this lab, I was able to:

- Understand security program maturity assessment frameworks and methodologies
- Install and configure a benchmarking environment on Linux (Ubuntu 24.04)
- Analyze security program maturity using standardized metrics
- Develop Python scripts to automate security benchmarking processes
- Generate maturity reports and identify improvement opportunities

---

## âœ… Prerequisites
- Basic knowledge of cybersecurity frameworks (NIST, ISO 27001)
- Linux command-line familiarity
- Python basics (functions, loops, file I/O)
- Understanding of governance and security program concepts

---

## ðŸ§° Lab Environment
- OS: **Ubuntu 24.04.1 LTS**
- User: **toor**
- Tools:
  - Python 3 + pip
  - python3-venv
  - nano editor
  - pandas, pyyaml, matplotlib, seaborn

---

## ðŸ“ Repository Structure
```text
lab07-security-benchmark/
â”œâ”€â”€ README.md
â”œâ”€â”€ commands.sh
â”œâ”€â”€ output.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â””â”€â”€ framework.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ questions.yaml
â”‚   â”œâ”€â”€ sample_responses.yaml
â”‚   â””â”€â”€ interactive_responses_YYYYMMDD_HHMMSS.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ benchmark_analyzer.py
â”‚   â”œâ”€â”€ report_generator.py
â”‚   â”œâ”€â”€ run_benchmark.py
â”‚   â”œâ”€â”€ interactive_assessment.py
â”‚   â””â”€â”€ compare_assessments.py
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ assessment_report.md
â”‚   â”œâ”€â”€ domain_scores.png
â”‚   â””â”€â”€ assessment_trend.png
â”œâ”€â”€ interview_qna.md
â””â”€â”€ troubleshooting.md
````

---

## ðŸ§© What This Lab Implements

### âœ… 1) Benchmarking Framework (YAML-driven)

You define:

* domains + weights
* maturity levels (1â€“5)
* thresholds for scoring bands

### âœ… 2) Question Bank + Weighted Scoring

Each domain has questions with internal weights.

### âœ… 3) Automated Scoring Engine

* calculates **domain maturity scores**
* calculates **overall weighted maturity**
* maps score â†’ **maturity level (1â€“5)**

### âœ… 4) Reporting

Generates:

* Markdown report: `reports/assessment_report.md`
* Bar chart PNG: `reports/domain_scores.png`

### âœ… 5) Interactive Assessments

Collect responses interactively and saves YAML into `data/`.

### âœ… 6) Trend Comparison

Compare multiple YAML assessments and generate:

* Trend chart: `reports/assessment_trend.png`

---

## â–¶ï¸ How To Run

### 1) Setup environment

```bash
sudo apt update && sudo apt install -y python3-pip python3-venv
mkdir -p ~/security-benchmark && cd ~/security-benchmark
python3 -m venv venv
source venv/bin/activate
pip install pandas pyyaml matplotlib seaborn
pip freeze > requirements.txt
```

### 2) Run benchmark analysis (sample)

```bash
python3 scripts/run_benchmark.py data/sample_responses.yaml
```

### 3) Run interactive assessment

```bash
python3 scripts/interactive_assessment.py
```

### 4) Compare multiple assessments

```bash
python3 scripts/compare_assessments.py data/sample_responses.yaml data/sample_responses.yaml
```

---

## ðŸ“Š Results & Key Metrics

This tool calculates:

* **Domain Scores** (0â€“100%)
* **Overall Weighted Score** (0â€“100%)
* **Maturity Level (1â€“5)** mapped from thresholds
* **Gap analysis** to highlight improvement priorities (lowest scoring domains)

Example computed output from sample responses:

* Overall Score: **63.9%**
* Maturity Level: **2 â€“ Developing**
* Lowest domains: **risk_management**, **incident_response**

---

## ðŸ”¥ Why This Matters (Real-World Relevance)

Security programs often improve slowly because maturity is not measured consistently.
This lab demonstrates how organizations can:

* Benchmark progress across security domains
* Identify weak areas objectively
* Track improvements over time
* Create repeatable, evidence-driven reporting for leadership

---

## âœ… Expected Outcomes Achieved

* âœ” Functional benchmarking environment
* âœ” YAML-based maturity framework
* âœ” Automated maturity scoring scripts
* âœ” Markdown reporting + visualization
* âœ” Interactive assessment collection
* âœ” Multi-assessment trend comparison

---

## ðŸ§  Conclusion

This lab demonstrates how maturity models provide structured measurement of a security program using standardized and weighted metrics.
By automating scoring + reporting, organizations can improve governance and prioritization decisions while tracking maturity growth over time.

